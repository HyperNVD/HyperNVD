import numpy as np
import torch
import cv2
import torch.optim as optim
import imageio

from PIL import Image
import matplotlib.patches as patches
import sys
import os

# sys.path.append('/ghome/mpilligua/video_editing/hypersprites-no-residual-no-hyphash/')

from utils.seg_utils import get_all_masks, compute_multiple_iou
from network.metamodules import get_subdict


def compute_consistency(flow12, flow21):
    wflow21 = warp_flow(flow21, flow12)
    diff = flow12 + wflow21
    diff = (diff[:, :, 0] ** 2 + diff[:, :, 1] ** 2) ** .5
    return diff


def warp_flow(img, flow):
    h, w = flow.shape[:2]
    flow = flow.copy()
    flow[:, :, 0] += np.arange(w)
    flow[:, :, 1] += np.arange(h)[:, np.newaxis]
    res = cv2.remap(img, flow, None, cv2.INTER_LINEAR)
    return res


def get_consistency_mask(optical_flow, optical_flow_reverse):
    mask_flow = compute_consistency(optical_flow.numpy(), optical_flow_reverse.numpy()) < 1.0
    mask_flow_reverse = compute_consistency(
        optical_flow_reverse.numpy(), optical_flow.numpy()) < 1.0
    return torch.from_numpy(mask_flow), torch.from_numpy(mask_flow_reverse)


def resize_flow(flow, newh, neww):
    oldh, oldw = flow.shape[0:2]
    flow = cv2.resize(flow, (neww, newh), interpolation=cv2.INTER_LINEAR)
    flow[:, :, 0] *= newh / oldh
    flow[:, :, 1] *= neww / oldw
    return flow


def load_input_data(
        resy, resx, maximum_number_of_frames,
        data_folder, use_mask_rcnn_bootstrapping,
        filter_optical_flow,
        sep_gtmask=True):
    '''
    data structure:
    └- data_folder
       |- video_frames
       |  └- png or jpg files
       |- masks
       |  |- folder0
       |  |  └- png or jpg files
       |  |- folder1
       |  |  └- png or jpg files
       |  └- ......
       └- flow
          └- npy files generated by preprocess_optical_flow.py
    '''
    video_frames_dir = data_folder / 'video_frames'
    flow_dir = data_folder / 'flow'
    mask_dirs = list((data_folder / 'masks').glob('*'))
    gt_mask_dir = data_folder / 'gt_mask'

    input_files = sorted(list(video_frames_dir.glob('*.jpg')) + list(video_frames_dir.glob('*.png')))
    number_of_frames=np.minimum(maximum_number_of_frames, len(input_files))
    
    gt_mask_frames = get_all_masks(gt_mask_dir, number_of_frames, resx, resy, separate_objects_masks=sep_gtmask)
    
    video_frames = torch.zeros((resy, resx, 3, number_of_frames))
    video_frames_dx = torch.zeros((resy, resx, 3, number_of_frames))
    video_frames_dy = torch.zeros((resy, resx, 3, number_of_frames))

    # objects + 1 background
    if use_mask_rcnn_bootstrapping:
        num_of_objects = len(mask_dirs)
    else:
        num_of_objects = 1

    mask_frames = torch.zeros((resy, resx, number_of_frames, num_of_objects+1))

    optical_flows = torch.zeros((resy, resx, 2, number_of_frames,  1))
    optical_flows_mask = torch.zeros((resy, resx, number_of_frames,  1))
    optical_flows_reverse = torch.zeros((resy, resx, 2, number_of_frames,  1))
    optical_flows_reverse_mask = torch.zeros((resy, resx, number_of_frames, 1))

    
    mask_files_list = [sorted(list(d.glob('*.jpg')) + list(d.glob('*.png'))) for d in mask_dirs]
    for i in range(number_of_frames):
        file1 = input_files[i]
        im = np.array(Image.open(str(file1))).astype(np.float64) / 255.
        if use_mask_rcnn_bootstrapping:
            for j, mask_files in enumerate(mask_files_list):
                # print(i, mask_files)
                mask = np.array(Image.open(str(mask_files[i]))).astype(np.float64) / 255.
                mask = cv2.resize(mask, (resx, resy), interpolation=cv2.INTER_NEAREST)
                mask_frames[:, :, i, j] = torch.from_numpy(mask)

        video_frames[:, :, :, i] = torch.from_numpy(cv2.resize(im[:, :, :3], (resx, resy)))
        video_frames_dy[:-1, :, :, i] = video_frames[1:, :, :, i] - video_frames[:-1, :, :, i]
        video_frames_dx[:, :-1, :, i] = video_frames[:, 1:, :, i] - video_frames[:, :-1, :, i]
    mask_frames[..., -1] = 1 - mask_frames.amax(dim=-1)


    for i in range(number_of_frames - 1):
        file1 = input_files[i]
        j = i + 1
        file2 = input_files[j]

        fn1 = file1.name
        fn2 = file2.name

        flow12_fn = flow_dir / f'{fn1}_{fn2}.npy'
        flow21_fn = flow_dir / f'{fn2}_{fn1}.npy'
        flow12 = np.load(flow12_fn)
        flow21 = np.load(flow21_fn)

        if flow12.shape[0] != resy or flow12.shape[1] != resx:
            flow12 = resize_flow(flow12, newh=resy, neww=resx)
            flow21 = resize_flow(flow21, newh=resy, neww=resx)
        mask_flow = compute_consistency(flow12, flow21) < 1.0
        mask_flow_reverse = compute_consistency(flow21, flow12) < 1.0

        optical_flows[:, :, :, i, 0] = torch.from_numpy(flow12)
        optical_flows_reverse[:, :, :, j, 0] = torch.from_numpy(flow21)

        if filter_optical_flow:
            optical_flows_mask[:, :, i, 0] = torch.from_numpy(mask_flow)
            optical_flows_reverse_mask[:, :, j, 0] = torch.from_numpy(mask_flow_reverse)
        else:
            optical_flows_mask[:, :, i, 0] = torch.ones_like(mask_flow)
            optical_flows_reverse_mask[:, :, j, 0] = torch.ones_like(mask_flow_reverse)
    return video_frames, video_frames_dx, video_frames_dy, optical_flows, optical_flows_reverse, optical_flows_mask, optical_flows_reverse_mask, mask_frames, gt_mask_frames




def get_tuples(number_of_frames, video_frames):
    # video_frames shape: (resy, resx, 3, num_frames),
    #  mask_frames shape: (resy, resx, num_frames)
    jif_all = []
    for f in range(number_of_frames):
        mask = (video_frames[:, :, :, f] > -1).any(dim=2)
        relis, reljs = torch.where(mask > 0.5)
        jif_all.append(torch.stack((reljs, relis, f * torch.ones_like(reljs))))
    return torch.cat(jif_all, dim=1)

# more efficient version 
# def get_tuples(number_of_frames=None, video_frames=None):
#     # video_frames shape: (resy, resx, 3, num_frames)
#     mask = (video_frames > -1).any(dim=2)  # shape: (resy, resx, num_frames)

#     # Find indices where mask is True for each frame
#     relis, reljs, relfs = torch.where(mask > 0.5)

#     # Stack all results similarly
#     jif_all = torch.stack((reljs, relis, relfs), dim=1)
    
#     # print the type of tensors
#     print(sys.getsizeof(jif_all))

#     return jif_all


def pre_train_mapping(model, 
                      index,
                      optimizer_mapping,
                      frames_num, 
                      uv_mapping_scale,
                      resx, resy, rez, 
                      device, 
                      pretrain_iters=100):
    # optimizer_mapping = optim.Adam(model_F_mapping.get_optimizer_list())
    # print(frames_num)
    # model_F_mapping.to(device)
    for i in range(pretrain_iters):
        for f in range(frames_num):
            i_s_int = torch.randint(resy, (1, np.int64(10000), 1))
            j_s_int = torch.randint(resx, (1, np.int64(10000), 1))

            i_s = i_s_int / (rez / 2) - 1
            j_s = j_s_int / (rez / 2) - 1

            xyt = torch.cat((
                j_s,
                i_s,
                (f / (frames_num / 2) - 1) * torch.ones_like(i_s)
            ), dim=2).to(device)

            output = model(xyt, index)
            uv_temp = output['uv']
            optimizer_mapping.zero_grad()

            loss = 0
            for l, uv in enumerate(uv_temp): 
                loss += (xyt[:, :, :2] * uv_mapping_scale[l] - uv).norm(dim=2).mean()
            loss.backward()
            optimizer_mapping.step()

            for l, uv in enumerate(uv_temp): 
                loss = 0
                loss = (xyt[:, :, :2] * uv_mapping_scale[l] - uv).norm(dim=2).mean()
                loss.backward()
                optimizer_mapping.step()

        if i % 10 == 0:
            print(f"pre-train loss: {loss.item()}")
    return model  #@donna  remember to check 



def pre_train_mapping_sep(model, optimizer_mapping, num_videos, num_frames, num_layers,
                          uv_mapping_scale, resx, resy, device, num_iters=100, wandb = None):
    
    rez = np.maximum(resx, resy)
    for i in range(num_iters):
        for index in range(num_videos):
            for l in range(num_layers): # for each of the objects / layers  
                for f in range(num_frames):
                    
                    i_s_int = torch.randint(resy, (1, np.int64(10000), 1))
                    j_s_int = torch.randint(resx, (1, np.int64(10000), 1))

                    i_s = i_s_int / (rez / 2) - 1
                    j_s = j_s_int / (rez / 2) - 1

                    f_s = (f / (num_frames / 2) - 1) * torch.ones_like(i_s) # why divided by 2?

                    xyt = torch.cat((j_s, i_s, f_s), dim=2).to(device)

                    index = torch.LongTensor([index]).to(device)


                    output = model(xyt, index)
                    uv_temp = output['uv'][l]
                
                    
                    optimizer_mapping.zero_grad()

                    loss = (xyt[:, :, :2] * uv_mapping_scale[l] - uv_temp).norm(dim=2).mean()
                    loss.backward()
                    optimizer_mapping.step()

                if i % 10 == 0:
                    print(f"pre-train loss for video {int(index)}: {loss.item()}")
                    wandb.log({'pre-train loss': loss.item(), 'video': int(index), 'layer': l, 'iter': i})
    return model  #@donna  remember to check 



def save_video(video_frames, results_folder):
    input_video = imageio.get_writer(
        "%s/input_video.mp4" % (results_folder), fps=10)
    for i in range(video_frames.shape[3]):
        cur_frame = video_frames[:, :, :, i].clone()
        input_video.append_data((cur_frame.numpy() * 255).astype(np.uint8))

    input_video.close()


def save_mask(masks_inital, masks_gt, num_frames, num_layers, results_folder):
    writers_error = [imageio.get_writer("%s/maskerror%d.mp4" % (results_folder, i), fps=10) for i in range(num_layers)]
    num_object = masks_gt.shape[-1]
    iou = np.zeros((num_frames, num_object))

    for t in range(num_frames):
        for i in range(num_object):
            pred_mask = masks_inital[:,:,t,i].numpy()
            gt_mask = masks_gt[:,:,t,i]
            iou[t,i] = compute_multiple_iou(pred_mask, gt_mask)
            error_map = np.zeros_like(pred_mask)
            error_map = np.where(pred_mask!=gt_mask, 1, 0)
            writers_error[i].append_data((error_map*255).astype(np.uint8))
    
    miou = iou.mean(axis=0)
    print('mious of the inital mask: ', miou)
    for i in writers_error: i.close()




from time import time
from datetime import timedelta

class Timer():
    def __init__(self):
        self.time_list = list()
        self.start_time = None

    def start(self):
        assert self.start_time is None
        self.start_time = time()

    def stop(self):
        assert self.start_time is not None
        self.time_list.append(time() - self.start_time)
        self.start_time = None

    def average(self):
        assert len(self.time_list) != 0
        return sum(self.time_list) / len(self.time_list)

    def last_period(self, period):
        assert len(self.time_list) >= period
        return sum(self.time_list[-period:]) / period

    def ETA(self, iterations, sample_num):
        assert len(self.time_list) >= sample_num
        ETA = self.last_period(sample_num) * iterations
        return timedelta(seconds=int(ETA))


import torch
import wandb
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def plot_point_cloud(xyt):
    # Assuming xyt is a tensor of shape (batch_size, num_points, 3)
    assert xyt.dim() == 3 and xyt.size(2) == 3, "Invalid shape for xyt tensor"
    
    # Extracting x, y, and t coordinates
    x = xyt[:,:,0].cpu().numpy().flatten()
    y = xyt[:,:,1].cpu().numpy().flatten()
    t = xyt[:,:,2].cpu().numpy().flatten()
    
    # Concatenating the coordinates
    points_rgb = np.column_stack((x, y, t, np.zeros_like(x), np.zeros_like(y), np.zeros_like(t)))
    
    # Log points in WandB
    wandb.log({
        "3d point cloud": wandb.Object3D({
            "type": "lidar/beta",
            "points": points_rgb
        })
    })


def plot_2d_points(xyt, id, xyt_gt):
    # Assuming xyt is a tensor of shape (batch_size, num_points, 3)
    assert xyt.dim() == 3 and xyt.size(2) == 2, "Invalid shape for xyt tensor"
    
    # Extracting x, y, and t coordinates
    x = xyt[:,:,0].cpu().detach().numpy().flatten()
    y = xyt[:,:,1].cpu().detach().numpy().flatten()
    # t = xyt[:,:,2].cpu().numpy().flatten()
    
    maxx = xyt_gt[:,:,0].max().item()
    minx = xyt_gt[:,:,0].min().item()
    
    maxy = xyt_gt[:,:,1].max().item()
    miny = xyt_gt[:,:,1].min().item()
    # print(maxx, minx, maxy, miny)
    
    # plot the points and save the plot
    plt.figure()
    rect  = patches.Rectangle((minx, miny), maxx-minx, maxy-miny, linewidth=0.5, edgecolor='r', facecolor='none')
    plt.scatter(x, y, s=0.1, cmap='viridis')
    plt.gca().add_patch(rect)
    os.makedirs('/data/users/mpilligua/plot_mapping', exist_ok=True)
    plt.savefig(f'/data/users/mpilligua/plot_mapping/{id}.png')
    plt.close()
    
    # data = [[xx, yy] for (xx, yy) in zip(x, y)]
    # table = wandb.Table(data=data, columns=["x", "y"])
    # wandb.log({"2d points": wandb.plot.scatter(table, "x", "y")})

if __name__=='__main__':
    # save the video stored in /data/datasets/DAVIS-2017/trainval-480p/DAVIS/JPEGImages/480p/car-turn
    # import os
    # import numpy as np
    
    # dir_video = '/data/datasets/DAVIS-2017/trainval-480p/DAVIS/JPEGImages/480p/car-turn'
    # video_frames = torch.zeros((768, 432, 3, len(os.listdir(dir_video))))
    
    # for i, file in enumerate(os.listdir(dir_video)):
    #     im = np.array(Image.open(os.path.join(dir_video, file))).astype(np.float64) / 255.
    #     video_frames[:, :, :, i] = torch.from_numpy(cv2.resize(im[:, :, :3], (432, 768)))
        
    # save_video(video_frames, '/data/users/mpilligua')
    
    device = 'cuda'
    
    resx = 768
    resy = 432
    rez = np.maximum(resx, resy)
    num_frames = 5
    num_layers = 2
    num_videos = 1
    uv_mapping_scale = [0.9, 0.9]
    f = 0
    index = 0
    
    i_s_int = torch.randint(resy, (1, np.int64(10000), 1))
    j_s_int = torch.randint(resx, (1, np.int64(10000), 1))

    i_s = i_s_int / (rez / 2) - 1
    j_s = j_s_int / (rez / 2) - 1

    f_s = (f / (num_frames / 2) - 1) * torch.ones_like(i_s) # why divided by 2?

    xyt = torch.cat((j_s, i_s, f_s), dim=2).to(device)

    index = torch.LongTensor([index]).to(device)

    
    xyt = torch.cat((j_s, i_s, f_s), dim=2).to(device)

    # Call the function to plot and log the plot
    with wandb.init(project='hypersprites', group='visualizations'):
        plot_point_cloud(xyt)